{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer SBM: Extension upon TopSBM \n",
    "\n",
    "A basic tutorial showing how to process Wikipedia data, fit a multilayer network and retrieve topics associated to consensus partition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in data\n",
    "\n",
    "First we retrieve the Wikipedia datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip \"./dataset-four.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import choice\n",
    "import scipy.stats\n",
    "import sys\n",
    "import gi\n",
    "from gi.repository import Gtk, Gdk\n",
    "import graph_tool.all as gt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "\n",
    "def retrieve_data(path_data):\n",
    "    \"\"\"\n",
    "    Retrieve the text, titles, mapping of articles to categories, graph construction,\n",
    "    and the true partition of the label.\n",
    "    \"\"\"\n",
    "\n",
    "    # Read in text data associated with each article\n",
    "    fname_data = 'processed/corpus.txt'\n",
    "    filename = os.path.join(path_data, fname_data)\n",
    "    with open(filename,'r', encoding = 'utf8') as f:\n",
    "        x = f.readlines()\n",
    "    texts = [h.split() for h in x]\n",
    "\n",
    "    # Read in titles data associated with each article\n",
    "    fname_data = 'processed/titles.txt'\n",
    "    filename = os.path.join(path_data, fname_data)\n",
    "    with open(filename,'r', encoding = 'utf8') as f:\n",
    "        x = f.readlines()\n",
    "    titles = [h.replace(\"\\n\", \"\") for h in x]\n",
    "\n",
    "    # We retrieve the Wikipedia label Category of each article\n",
    "    category_titles_path = \"raw/corpus_categoryTitle.csv\"\n",
    "    category_title = pd.read_csv(os.path.join(path_data, category_titles_path), sep = \"\\t\")\n",
    "\n",
    "    # Read in hyperlink graph data\n",
    "    hyperlink_edgelist = 'processed/hyperlink_edgelist.csv'\n",
    "    hyperlink_g = gt.load_graph_from_csv(os.path.join(path_data, hyperlink_edgelist),\n",
    "                              skip_first=True,\n",
    "                              directed=True,\n",
    "                              csv_options={'delimiter': '\\t', 'quotechar': '\"'})\n",
    "    num_vertices = hyperlink_g.num_vertices()\n",
    "    num_edges = hyperlink_g.num_edges()\n",
    "\n",
    "    # Create hyperlinks list\n",
    "    filename = os.path.join(path_data, hyperlink_edgelist)\n",
    "    x = pd.read_csv(filename, delimiter = \"\\t\")\n",
    "    hyperlinks = [(row[0],row[1]) for source, row in x.iterrows()]\n",
    "\n",
    "    # Assign label based on category to each wikipedia article node.\n",
    "    # Dict mapping between wikipedia article and category\n",
    "    article_category = {}\n",
    "    for index, row in category_title.iterrows():\n",
    "        article_category[row[2]] = row[0]\n",
    "\n",
    "    # Category types in this dataset\n",
    "    print(set(category_title.category))\n",
    "\n",
    "    # We need to assign wikipedia category to every wiki article\n",
    "    label = hyperlink_g.vp[\"label\"] = hyperlink_g.new_vp(\"string\")\n",
    "    name = hyperlink_g.vp[\"name\"] # every vertex has a name already associated to it!\n",
    "\n",
    "    # We now assign category article to each Wikipedia article\n",
    "    for v in hyperlink_g.vertices():\n",
    "        category_of_article = article_category[name[v]]\n",
    "        label[v] = category_of_article # assign wikipedia category to article\n",
    "        \n",
    "    # Retrieve true partition of graph\n",
    "    true_partition = list(hyperlink_g.vp.label)    \n",
    "    # Retrieve ordering of articles\n",
    "    article_names = list(hyperlink_g.vp.name)\n",
    "\n",
    "    # Remove parallel edges in hyperlink graph\n",
    "    gt.remove_parallel_edges(hyperlink_g)\n",
    "    unique_hyperlinks = []\n",
    "    for e in hyperlinks:\n",
    "        if e not in unique_hyperlinks:\n",
    "            unique_hyperlinks.append(e)\n",
    "\n",
    "    return texts, titles, category_title, article_category, hyperlink_g, unique_hyperlinks, label, name, true_partition, article_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = f\"./dataset-four/data/dataset-four\"\n",
    "texts, titles, category_title, article_category, hyperlink_g, hyperlinks, label, name,true_partition, article_names = retrieve_data(path_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words in text data\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "edited_text = []\n",
    "# Recall texts is list of lists of words in each document.\n",
    "for doc in texts:\n",
    "    temp_doc = []\n",
    "    for word in doc:\n",
    "        if word not in stopwords.words('english'):\n",
    "            temp_doc.append(word)\n",
    "    edited_text.append(temp_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display what original network with Wikipedia labels look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Original Wikipedia partition\")\n",
    "# To assign color to each node.\n",
    "vertex_color = hyperlink_g.vp[\"vertex_color\"] = hyperlink_g.new_vp(\"string\")\n",
    "pos = hyperlink_g.vp[\"pos\"] = gt.sfdp_layout(hyperlink_g)\n",
    "\n",
    "# Establish color palette for graph by mapping category to color.\n",
    "colors = {}\n",
    "palette = [\"lightskyblue\", \"lightyellow\", \"lightpink\"] # ASSUMING 3 CATEGORIES!\n",
    "for cat, col in zip(set(article_category.values()), palette):\n",
    "    colors[cat] = col\n",
    "\n",
    "# Assign color palette to each node\n",
    "for v in hyperlink_g.vertices():\n",
    "    vertex_color[v] = colors[label[v]] # retrieve category of article and retrieve color\n",
    "\n",
    "# Visualise graph\n",
    "gt.graph_draw(hyperlink_g,\n",
    "              pos=hyperlink_g.vp[\"pos\"],\n",
    "              vertex_fill_color=hyperlink_g.vp.vertex_color, \n",
    "              inline = True)\n",
    "print(\"Color palette is\")\n",
    "print(colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Fitting "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitting the Hyperlink + Text model\n",
    "\n",
    "We now fit a 2-layer SBM with one layer being the text layer (see TopSBM) composing of a bipartite network of document and word nodes whilst the hyperlink layer consists of hyperlinks between document nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sbmmultilayer import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED_NUM = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_hyperlink_text_hsbm(edited_text, titles, hyperlinks, N_iter):\n",
    "    \"\"\"\n",
    "    Fit N_iter iterations of doc-network sbm on dataset through agglomerative heuristic\n",
    "    and simulated annealing.\n",
    "    \"\"\"\n",
    "    hyperlink_text_hsbm_post = []\n",
    "    hyperlink_text_hsbm_result = []\n",
    "\n",
    "    for _ in range(N_iter):\n",
    "        print(f\"Iteration {_}\")\n",
    "        # Construct 2-layer network hyperlink-text model and fit multilayer SBM.\n",
    "        hyperlink_text_hsbm = sbmmultilayer(random_seed=SEED_NUM)\n",
    "        hyperlink_text_hsbm.make_graph(edited_text, titles, hyperlinks)\n",
    "        hyperlink_text_hsbm.fit()\n",
    "\n",
    "        # Retrieve state from simulated annealing hSBM\n",
    "        hyperlink_text_hsbm_post_state, results_sim_anneal = run_multiflip_simulated_annealing_hsbm(hyperlink_text_hsbm)\n",
    "\n",
    "        # Update hSBM model using state from simulated annealing\n",
    "        updated_hsbm_model = hyperlink_text_hsbm\n",
    "        updated_hsbm_model.state = hyperlink_text_hsbm_post_state\n",
    "        updated_hsbm_model.mdl = hyperlink_text_hsbm_post_state.entropy()\n",
    "        updated_hsbm_model.n_levels = len(hyperlink_text_hsbm_post_state.levels)\n",
    "\n",
    "        # Save the results from simulated annealing\n",
    "        hyperlink_text_hsbm_post.append(updated_hsbm_model)\n",
    "        hyperlink_text_hsbm_result.append(results_sim_anneal)\n",
    "\n",
    "    return hyperlink_text_hsbm_post, hyperlink_text_hsbm_result \n",
    "\n",
    "\n",
    "def run_multiflip_simulated_annealing_hsbm(hsbm_model):\n",
    "    \"\"\"\n",
    "    Run multiflip simualted annealing on multilayer SBM.\n",
    "    Return:\n",
    "        hsbm_state_copy - State associated to SBM at the end of simulated annealing.\n",
    "        final_results_multiflip - (nattempts, nmoves, entropy) associated to simulated annealing.\n",
    "    \"\"\"\n",
    "    S1 = hsbm_model.mdl\n",
    "    print(f\"Initial entropy is {S1}\")\n",
    "\n",
    "    # We create a copy and perform simulated annealing on the copy to retain\n",
    "    # original hSBM state.\n",
    "    init_hsbm_state = hsbm_model.state\n",
    "\n",
    "    # Allows for higher levels of groups to be formed\n",
    "    hsbm_state_copy = init_hsbm_state.copy(bs=init_hsbm_state.get_bs() + [np.zeros(1)]*4, sampling=True)\n",
    "\n",
    "    # Fit simulated annealing and store results.\n",
    "    final_results_multiflip = []\n",
    "    results_multiflip = gt.mcmc_equilibrate(hsbm_state_copy, force_niter=1000, mcmc_args=dict(beta=1),history=True)\n",
    "    final_results_multiflip.extend(results_multiflip)\n",
    "    results_multiflip = gt.mcmc_equilibrate(hsbm_state_copy, force_niter=4000, mcmc_args=dict(beta=np.inf),history=True)    \n",
    "    final_results_multiflip.extend(results_multiflip)\n",
    "\n",
    "    S2 = hsbm_state_copy.entropy()\n",
    "    print(f\"New entropy is {S2}\")\n",
    "    print(f\"Improvement after simulated annealing {S2 - S1}\")\n",
    "    print(f\"The improvement percentage is { ((S2 - S1)/S1) * 100 }\")\n",
    "\n",
    "    return hsbm_state_copy, final_results_multiflip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute multiple runs of fitting multilayer SBM with simualted annealing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "hyperlink_text_hsbm_states, hyperlink_text_hsbm_results =  fit_hyperlink_text_hsbm(edited_text, titles, hyperlinks, 3)\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise Results\n",
    "\n",
    "Visualise the results from simulated annealing. Examine the description length and autocorrelation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "def visualise_multiflip_sim_annealing(results):\n",
    "    \"\"\"\n",
    "    Visualise the multiflip simulated annealing by analysing the description length.\n",
    "    \"\"\"    \n",
    "    # Retrieve figures needed to plot MDL\n",
    "    # Retrieve MDL data from multiflip function\n",
    "    fig, ax = plt.subplots(1, figsize = (15, 10))\n",
    "    count_i = 0\n",
    "    for results_sim_anneal in results:\n",
    "        mdls = []\n",
    "        mdl_temp = []\n",
    "        for row in range(len(results_sim_anneal)):\n",
    "            mdls.append(results_sim_anneal[row][2])\n",
    "\n",
    "        # Compute mean and std for plotting.\n",
    "        mdl_averages = [np.average(mdl) for mdl in mdls]\n",
    "        mdl_std = [np.std(mdl) for mdl in mdls]\n",
    "        plt.plot(range(len(results_sim_anneal)), mdls,\"^\", markersize=1, label=count_i)\n",
    "        count_i += 1\n",
    "    plt.xlabel(\"MCMC Sweeps\")\n",
    "    plt.ylabel(r\"Description length $\\Sigma$\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Description length of MCMC over sweeps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualise_multiflip_sim_annealing(hyperlink_text_hsbm_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc3 as pm3\n",
    "\n",
    "LAGS_LIMIT = 1500\n",
    "\n",
    "def compute_autocorrelation_plot(results, model):\n",
    "    \"\"\"   \n",
    "    Plot the autocorrelation plot to show mixing time.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots()\n",
    "    for iteration in range(len(results)):\n",
    "        mdls = [entry[2] for entry in results[iteration]] # this is the MDL for each realisation\n",
    "        \n",
    "        lags = np.arange(1, LAGS_LIMIT)\n",
    "        values = np.array(mdls)    \n",
    "        ax.plot(lags, [pm3.autocorr(values, l) for l in lags])\n",
    "    _ = ax.set(xlabel=r'$\\tau$ Lag', ylabel='Autocorrelation', ylim=(-.1, 1))\n",
    "    plt.title(f'Autocorrelation Plot for {model} model')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_autocorrelation_plot(hyperlink_text_hsbm_results, \"Hyperlink + Text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve partitions\n",
    "\n",
    "Retrieve the partitions assigned to the document nodes by examining the highest non-trivial level of the hierarchical degree-corrected SBM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils.nmi import *\n",
    "from Utils.doc_clustering import *\n",
    "\n",
    "def find_highest_non_trivial_group(num_levels, curr_hsbm):\n",
    "    \"\"\"\n",
    "    Find the highest group that is not just 1 group.\n",
    "    \"\"\"\n",
    "    top_levels = curr_hsbm.n_levels\n",
    "    temp_l = 0\n",
    "    highest_l = [temp_l]\n",
    "    final_l = 0\n",
    "    \n",
    "    # Compute values of interest.\n",
    "    clustering_info = doc_clustering(\"./\", hyperlink_g)\n",
    "    clustering_info.seeds = [0] # we only want 1 iteration.\n",
    "\n",
    "    for i in range(top_levels):\n",
    "        # Iterate until no longer high enough level.\n",
    "        highest_l = [temp_l]\n",
    "        doc_partitions, doc_num_groups, doc_entropies = clustering_info.collect_info(\"1-layer-doc\", curr_hsbm.g, highest_l, curr_hsbm.state)\n",
    "        if doc_num_groups == [1]:\n",
    "            # We have found highest level we can go.\n",
    "            final_l = i-1\n",
    "            break\n",
    "        # Still on non-trivial level, so still continue\n",
    "        temp_l += 1\n",
    "    highest_l = [final_l]\n",
    "    doc_partitions, doc_num_groups, doc_entropies = clustering_info.collect_info(\"1-layer-doc\", curr_hsbm.g, highest_l, curr_hsbm.state)\n",
    "    print(f\"We chose level {final_l} out of levels {num_levels}\")\n",
    "    print(\"Number of groups\", doc_num_groups)\n",
    "    print(\"\\n\")\n",
    "    return doc_partitions, final_l\n",
    "\n",
    "\n",
    "def get_hsbm_partitions(model_states):\n",
    "    \"\"\"\n",
    "    For each mu, retrieve the partitions.\n",
    "    \"\"\"\n",
    "    model_partitions = []\n",
    "    levels = [] # store highest non-trivial level\n",
    "    # Retrieve partitions\n",
    "    for i in range(len(model_states)):\n",
    "        curr_hsbm = model_states[i] #retrieve hSBM model       \n",
    "        # Figure out top layer\n",
    "        top_levels = curr_hsbm.n_levels\n",
    "        print(\"Number of levels\", top_levels)\n",
    "        model_partition, highest_level = find_highest_non_trivial_group(top_levels, curr_hsbm)\n",
    "        model_partitions.append(model_partition[0])\n",
    "        levels.append(highest_level)\n",
    "    return model_partitions, levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve partitions assigned to documents in each run. Also save index of highest non-trivial level.\n",
    "hyperlink_text_hsbm_partitions, levels = get_hsbm_partitions(hyperlink_text_hsbm_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consensus Partition\n",
    "\n",
    "Compute the consensus partition assignment to document nodes over all the runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperlink_text_consensus_partitions, hyperlink_text_consensus_partitions_sd = gt.partition_overlap_center(hyperlink_text_hsbm_partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remap_color_partition_manually(partition, color_plate):\n",
    "    \"\"\"\n",
    "    Manually map colors to partitions for visualisation.\n",
    "    \"\"\"\n",
    "    block_groups = list(set(partition))\n",
    "    color_map = {}\n",
    "    for i in range(len(block_groups)):\n",
    "        color_map[block_groups[i]] = color_plate[i]\n",
    "\n",
    "    return color_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Original Wikipedia partition\")\n",
    "# To assign color to each node.\n",
    "vertex_color = hyperlink_g.vp[\"vertex_color\"] = hyperlink_g.new_vp(\"string\")\n",
    "pos = hyperlink_g.vp[\"pos\"] = gt.sfdp_layout(hyperlink_g)\n",
    "\n",
    "# Establish color palette for graph by mapping category to color.\n",
    "colors = {}\n",
    "palette = [\"lightskyblue\", \"lightyellow\", \"lightpink\"] # ASSUMING 3 CATEGORIES!\n",
    "for cat, col in zip(set(article_category.values()), palette):\n",
    "    colors[cat] = col\n",
    "\n",
    "# Assign color palette to each node\n",
    "for v in hyperlink_g.vertices():\n",
    "    vertex_color[v] = colors[label[v]] # retrieve category of article and retrieve color\n",
    "\n",
    "# Visualise graph\n",
    "gt.graph_draw(hyperlink_g,\n",
    "              pos=hyperlink_g.vp[\"pos\"],\n",
    "              vertex_fill_color=hyperlink_g.vp.vertex_color, \n",
    "              inline = True)\n",
    "print(\"Color palette is\")\n",
    "print(colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualise consensus partition from hyperlink + text model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new vertex property\n",
    "vertex_color = hyperlink_g.vp[\"vertex_color\"] = hyperlink_g.new_vp(\"string\")\n",
    "\n",
    "print(\"Hyperlink + Text Consensus Partition\")\n",
    "print(\"Uncertainty associated to consensus partition is \", hyperlink_text_consensus_partitions_sd)\n",
    "# Color doc network.\n",
    "colors = [\"lightskyblue\", \"lightpink\", \"lightgreen\", \"lightyellow\"]\n",
    "colormap = remap_color_partition_manually(hyperlink_text_consensus_partitions, colors)\n",
    "\n",
    "# Assign a value to that property for each node of that graph\n",
    "for v, group in zip(hyperlink_g.vertices(), hyperlink_text_consensus_partitions):\n",
    "    vertex_color[v] = colormap[group]\n",
    "\n",
    "gt.graph_draw(hyperlink_g,\n",
    "              pos=hyperlink_g.vp[\"pos\"],\n",
    "              vertex_fill_color=hyperlink_g.vertex_properties['vertex_color'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partition overlap and normalised mutual information\n",
    "\n",
    "Compute heatmap showing the partition overlap and normalised mutual information of hyperlink + text model and Wikipedia labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from Utils.doc_clustering import *\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "sns.set(font_scale=2)\n",
    "\n",
    "def visualise_partition_overlap_results(partition_overlap_avg, partition_overlap_std , cols, title):\n",
    "    \"\"\"\n",
    "    Construct heat-map to show partition overlap matrix.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    sns.set(font_scale=2)\n",
    "    \n",
    "    mask = np.zeros_like(partition_overlap_avg)\n",
    "    mask[np.triu_indices_from(mask, k=1)] = True\n",
    "    final_df = pd.DataFrame(partition_overlap_avg, columns=cols, index=cols)\n",
    "    labels = (np.asarray([\"{0:.2f}\\n({1:.2f})\".format(partition_overlap_avg_entry, partition_overlap_std_entry) for partition_overlap_avg_entry, partition_overlap_std_entry in zip(partition_overlap_avg.flatten(), partition_overlap_std.flatten())])).reshape(len(partition_overlap_avg), len(partition_overlap_avg))    \n",
    "    ax = sns.heatmap(data=final_df, annot = labels, mask=mask, linewidth=0.5, square=True, cmap=\"Blues\", fmt='')\n",
    "    ax.set_title(title)\n",
    "    locs, labels = plt.yticks()\n",
    "    plt.setp(labels, rotation=0)\n",
    "    sns.axes_style(\"white\")\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_partition_overlap(partition_i, partition_j):\n",
    "    \"\"\"\n",
    "    Compute the maximum partition overlap between the two partitions.\n",
    "    \"\"\"\n",
    "    return gt.partition_overlap(partition_i, partition_j)\n",
    "\n",
    "\n",
    "def _max_overlap_partition(partitions):\n",
    "    \"\"\"\n",
    "    Helper function for calculating the partition overlap. Take in list of partitions from 2 different models and\n",
    "    computes the partition overlap between the two models' partitions.\n",
    "\n",
    "    partitions: list of list\n",
    "    The partitions will be two lists of lists of size N_ITER where each list corresponds to partitions of a model \n",
    "    where we retrieve N_ITER partitions each time.\n",
    "\n",
    "    Return: list of partition overlap between partitions.\n",
    "    \"\"\"\n",
    "    n = len(partitions) # 10 * 10, depends on number of iterations to retrieve partitions\n",
    "    overlap_partition_matrix = np.zeros((n,n))\n",
    "    for i in range(n):\n",
    "        for j in range(i+1,n):\n",
    "            overlap_partition_matrix[i,j] = compute_partition_overlap(partitions[i], partitions[j])\n",
    "    return list(overlap_partition_matrix[np.triu_indices(n,1)]) # return upper triangle for array.\n",
    "\n",
    "def construct_maximum_overlap_partition_matrix(partitions, true_partition):\n",
    "    \"\"\"\n",
    "    Compute maximum overlap partition matrix for all partitions generated by models against true partition.\n",
    "\n",
    "    partitions: list of list of partitions for each model. \n",
    "    true_partition: single list of true partitions.\n",
    "\n",
    "    Remark: For example, we may generate 20 different partitions for the hSBM and compare it to the true partitions.\n",
    "    First column is for the true partition.\n",
    "    \"\"\"\n",
    "    num_models = len(partitions) # number of different models we are testing.\n",
    "    # Store the average and standard deviation of partition overlap between partitions in a n x n matrix\n",
    "    max_partition_overlap_avg = np.zeros((num_models+1, num_models+1))\n",
    "    max_partition_overlap_std = np.zeros((num_models+1, num_models+1))\n",
    "\n",
    "    # Iterate through NMI matrix and compute partition overlap between models excluding the ground truth.\n",
    "    # We do not iterate through the first column.\n",
    "    for i in range(1, num_models+1):\n",
    "        for j in range(i, num_models+1):\n",
    "            partition_overlaps = _max_overlap_partition(partitions[i-1] + partitions[j-1]) # retrieve list of partitions for model i-1 and model j-1\n",
    "            # Store mean and std of partition overlap.\n",
    "            max_partition_overlap_avg[i,j] = np.average(partition_overlaps)\n",
    "            max_partition_overlap_std[i,j] = np.std(partition_overlaps)\n",
    "\n",
    "    max_partition_overlap_avg[0,0], max_partition_overlap_std[0,0] = 1, 0 # true partition should have NMI of 1 with itself.\n",
    "    # Compute the NMI for each model against ground truth. Corresponds to 1st column of NMI matrix.\n",
    "    for i in range(num_models):\n",
    "        # Compute NMI of model's partition with ground truth labels.\n",
    "        partition_overlap_with_true = [compute_partition_overlap(p, true_partition) for p in partitions[i]]\n",
    "        max_partition_overlap_avg[0, i+1] = np.average(partition_overlap_with_true)\n",
    "        max_partition_overlap_std[0, i+1] = np.std(partition_overlap_with_true)\n",
    "    return (max_partition_overlap_avg.T, max_partition_overlap_std.T)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_partition_to_int(partition):\n",
    "    # Helper function for Wikipedia labelling for heatmap.\n",
    "    cat_map = {}\n",
    "    for id, cat in enumerate(set(partition)):\n",
    "        cat_map[cat] = id\n",
    "\n",
    "    int_mapping = []\n",
    "    for entry in partition:\n",
    "        int_mapping.append( cat_map[entry] )\n",
    "\n",
    "    return int_mapping\n",
    "\n",
    "true_partition_int = map_partition_to_int(true_partition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the average partition overlap and standard deviation\n",
    "max_partition_overlap_avg, max_partition_overlap_std = construct_maximum_overlap_partition_matrix([hyperlink_text_hsbm_partitions], true_partition_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = [\"Wiki-Labels\", \"H+T\"]\n",
    "visualise_heatmap_results(max_partition_overlap_avg, max_partition_overlap_std, column_names, r\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils.doc_clustering import *\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "sns.set(font_scale=2)\n",
    "\n",
    "def visualise_heatmap_results(averages, std , cols, title):\n",
    "    \"\"\"\n",
    "    Construct heat-map to show NMI of partitions.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    sns.axes_style(\"white\")\n",
    "    sns.set_style(\"whitegrid\")\n",
    "\n",
    "    mask = np.zeros_like(averages)\n",
    "    mask[np.triu_indices_from(mask, k=1)] = True\n",
    "    final_df = pd.DataFrame(averages, columns=cols, index=cols)\n",
    "    labels = (np.asarray([\"{0:.2f}\\n({1:.2f})\".format(averages_entry, std_entry) for averages_entry, std_entry in zip(averages.flatten(), std.flatten())])).reshape(len(averages), len(averages))    \n",
    "    ax = sns.heatmap(data=final_df, annot = labels, mask=mask, linewidth=0.5, square=True, cmap=\"Blues\", fmt='')\n",
    "    ax.set_title(title)\n",
    "    locs, labels = plt.yticks()\n",
    "    plt.setp(labels, rotation=0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the average NMI and standard deviation\n",
    "nmi_avg, nmi_std = construct_nmi_matrix([hyperlink_text_hsbm_partitions], true_partition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = [\"Wiki-Labels\", \"H+T\"]\n",
    "visualise_heatmap_results(nmi_avg, nmi_std, column_names, r\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modelling\n",
    "\n",
    "We now show how this framework tackles the problem of topic modelling simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get topics for one run of SBM at highest non-trivial level\n",
    "hyperlink_text_hsbm_states[0].get_topics(l=2, n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_num = 0\n",
    "print(hyperlink_text_hsbm_states[0].documents[doc_num]) # see document name\n",
    "# Retrieve proportion of topics for document 0\n",
    "hyperlink_text_hsbm_states[0].get_topicProportion(doc_num, l=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now retrieve the topics associated to the consensus partition of Hyperlink + Text model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_type_blocks(h_t_state, h_t_graph, level):\n",
    "    \"\"\"\n",
    "    Retrieve the block assignment of WORD types for H+T model.\n",
    "    \"\"\"\n",
    "    partitions = []\n",
    "    num_of_groups = []\n",
    "    entropies = []\n",
    "    block_SBM_partitions = {} # store dictionary to map nodes to partition.\n",
    "    b = h_t_state.project_level(level).get_blocks()\n",
    "    # Need to specify to retrieve partitions for WORD type nodes.\n",
    "    for node in h_t_graph.vertices():\n",
    "        if h_t_graph.vp['kind'][node] == 1:\n",
    "            block_SBM_partitions[h_t_graph.vp.name[node]] = b[node]                    \n",
    "    \n",
    "    # Retrieve the partition from the SBM and store as parameter.    \n",
    "    partition = h_t_graph.vp[\"partition\"] = h_t_graph.new_vp(\"int\")\n",
    "    # Assign partition label to node properties.\n",
    "    for v in h_t_graph.vertices():\n",
    "        if h_t_graph.vp['kind'][v] == 1:\n",
    "            partition[v] = block_SBM_partitions[h_t_graph.vp.name[v]]\n",
    "    # IGNORE FIRST 120 NODES (there are document nodes)\n",
    "    partitions.append(list(h_t_graph.vp.partition)[120:])\n",
    "    num_of_groups.append(len(set(partitions[0])))\n",
    "    entropies.append(h_t_state.entropy())\n",
    "    return (partitions, num_of_groups, entropies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H_T_word_hsbm_partitions = []\n",
    "H_T_word_hsbm_num_groups = []\n",
    "for i in range(len(levels)):\n",
    "    print(i)\n",
    "    word_partitions, num_word_groups, en = get_word_type_blocks(hyperlink_text_hsbm_states[i].state, hyperlink_text_hsbm_states[i].g, levels[i])\n",
    "    H_T_word_hsbm_partitions.append(word_partitions[0])\n",
    "    H_T_word_hsbm_num_groups.append(num_word_groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now retrieve the consensus partitions for the document and word type nodes respectively. We now \"count\" the number of edges between document clusters and word type groups (i.e. topics) in order to compute the distributions required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_t_doc_consensus = gt.partition_overlap_center(hyperlink_text_hsbm_partitions)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = len((h_t_doc_consensus)) # number of document nodes\n",
    "\n",
    "h_t_word_consensus = gt.partition_overlap_center(H_T_word_hsbm_partitions)[0]\n",
    "h_t_word_consensus += len(set(h_t_doc_consensus)) # to get cluster number to not start from 0\n",
    "\n",
    "V = len((h_t_word_consensus)) # number of word-type nodes\n",
    "# number of word-tokens (edges excluding hyperlinks)\n",
    "N = int(np.sum([hyperlink_text_hsbm_states[0].g.ep.edgeCount[e] for e in hyperlink_text_hsbm_states[0].g.edges() if hyperlink_text_hsbm_states[0].g.ep['edgeType'][e]== 0 ])) \n",
    "\n",
    "# Number of blocks\n",
    "B = len(set(h_t_word_consensus)) + len(set(h_t_doc_consensus))\n",
    "\n",
    "# Count labeled half-edges, total sum is # of edges\n",
    "# Number of half-edges incident on word-node w and labeled as word-group tw\n",
    "n_wb = np.zeros((V,B)) # will be reduced to (V, B_w)\n",
    "\n",
    "# Number of half-edges incident on document-node d and labeled as document-group td\n",
    "n_db = np.zeros((D,B)) # will be reduced to (D, B_d)\n",
    "\n",
    "# Number of half-edges incident on document-node d and labeled as word-group tw\n",
    "n_dbw = np.zeros((D,B))  # will be reduced to (D, B_w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All graphs created the same for each H+T model\n",
    "for e in hyperlink_text_hsbm_states[0].g.edges():\n",
    "    # Each edge will be between a document node and word-type node\n",
    "    if hyperlink_text_hsbm_states[0].g.ep.edgeType[e] == 0:        \n",
    "        # v1 ranges from 0, 1, 2, ..., D - 1\n",
    "        # v2 ranges from D, ..., (D + V) - 1 (V # of word types)\n",
    "        v1 = int(e.source()) # document node index\n",
    "        v2 = int(e.target()) # word type node index\n",
    "        # z1 will have values from 1, 2, ..., B_d; document-group i.e document block that doc node is in \n",
    "        # z2 will have values from B_d + 1, B_d + 2,  ..., B_d + B_w; word-group i.e word block that word type node is in\n",
    "        # Recall that h_t_word_consensus starts at 0 so need to -120\n",
    "        z1, z2 = h_t_doc_consensus[v1], h_t_word_consensus[v2-120]\n",
    "        n_wb[v2-D,z2] += 1 # word type v2 is in topic z2\n",
    "        n_db[v1,z1] += 1 # document v1 is in doc cluster z1\n",
    "        n_dbw[v1,z2] += 1 # document v1 has a word in topic z2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_db = n_db[:, np.any(n_db, axis=0)] # (D, B_d)\n",
    "n_wb = n_wb[:, np.any(n_wb, axis=0)] # (V, B_w)\n",
    "n_dbw = n_dbw[:, np.any(n_dbw, axis=0)] # (D, B_d)\n",
    "\n",
    "B_d = n_db.shape[1]  # number of document groups\n",
    "B_w = n_wb.shape[1] # number of word groups (topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group membership of each word-type node in topic, matrix of ones or zeros, shape B_w x V\n",
    "# This tells us the probability of topic over word type\n",
    "p_tw_w = (n_wb / np.sum(n_wb, axis=1)[:, np.newaxis]).T\n",
    "\n",
    "# Group membership of each doc-node, matrix of ones of zeros, shape B_d x D\n",
    "p_td_d = (n_db / np.sum(n_db, axis=1)[:, np.newaxis]).T\n",
    "\n",
    "# Mixture of word-groups into documents P(t_w | d), shape B_d x D\n",
    "p_tw_d = (n_dbw / np.sum(n_dbw, axis=1)[:, np.newaxis]).T\n",
    "\n",
    "# Per-topic word distribution, shape V x B_w\n",
    "p_w_tw = n_wb / np.sum(n_wb, axis=0)[np.newaxis, :]\n",
    "\n",
    "\n",
    "h_t_consensus_summary = {}\n",
    "h_t_consensus_summary['Bd'] = B_d # Number of document groups\n",
    "h_t_consensus_summary['Bw'] = B_w # Number of word groups\n",
    "h_t_consensus_summary['p_tw_w'] = p_tw_w # Group membership of word nodes\n",
    "h_t_consensus_summary['p_td_d'] = p_td_d # Group membership of document nodes\n",
    "h_t_consensus_summary['p_tw_d'] = p_tw_d # Topic proportions over documents\n",
    "h_t_consensus_summary['p_w_tw'] = p_w_tw # Topic distribution over words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topics_h_t_consensus_model(groups, words, n=10):\n",
    "    \"\"\"\n",
    "    Retrieve topics in consensus partition for H+T model.\n",
    "    \"\"\"\n",
    "    dict_groups = groups\n",
    "    Bw = dict_groups['Bw'] # number of word-groups\n",
    "    p_w_tw = dict_groups['p_w_tw'] # topic proportions over documents\n",
    "    words = words\n",
    "    # Loop over all word-groups\n",
    "    dict_group_words = {}\n",
    "    for tw in range(Bw):\n",
    "        p_w_ = p_w_tw[:, tw]\n",
    "        ind_w_ = np.argsort(p_w_)[::-1]\n",
    "        list_words_tw = []\n",
    "        for i in ind_w_[:n]:\n",
    "            if p_w_[i] > 0:\n",
    "                list_words_tw+=[(words[i],p_w_[i])]\n",
    "            else:\n",
    "                break\n",
    "        dict_group_words[tw] = list_words_tw\n",
    "    return dict_group_words    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_words = [ hyperlink_text_hsbm_states[0].g.vp['name'][v] for v in  hyperlink_text_hsbm_states[0].g.vertices() if hyperlink_text_hsbm_states[0].g.vp['kind'][v]==1   ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_groups = get_topics_h_t_consensus_model(h_t_consensus_summary, g_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write out topics as dataframe\n",
    "topic_csv_dict = {}\n",
    "for key in dict_groups.keys():\n",
    "    topic_csv_dict[key] = [entry[0] for entry in dict_groups[key]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = topic_csv_dict.keys()\n",
    "topics_df = pd.DataFrame()\n",
    "\n",
    "for key in dict_groups.keys():\n",
    "    temp_df = pd.DataFrame(topic_csv_dict[key], columns=[key])\n",
    "    topics_df = pd.concat([topics_df, temp_df], ignore_index=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now retrieve the top 10 words associated to topics associated to consensus partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
